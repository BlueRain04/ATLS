{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d95068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from resco_benchmark.agents.agent import Agent, IndependentAgent\n",
    "from resco_benchmark.config.signal_config import signal_configs\n",
    "\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    tf = None\n",
    "    pass\n",
    "\n",
    "\n",
    "if tf is None:\n",
    "    class MA2C(IndependentAgent):\n",
    "        def __init__(self, config, obs_act, map_name, thread_number, sess=None):\n",
    "            super().__init__(config, obs_act, map_name, thread_number)\n",
    "            raise EnvironmentError(\"Install optional tensorflow requirement for MA2C\")\n",
    "\n",
    "else:\n",
    "\n",
    "    class MA2C(IndependentAgent):\n",
    "        def __init__(self, config, obs_act, map_name, thread_number, sess=None):\n",
    "            super().__init__(config, obs_act, map_name, thread_number)\n",
    "\n",
    "            self.signal_config = signal_configs[map_name]\n",
    "\n",
    "            if sess is None:\n",
    "                tf.reset_default_graph()\n",
    "                cfg_proto = tf.ConfigProto(allow_soft_placement=True)\n",
    "                self.sess = tf.Session(config=cfg_proto)\n",
    "\n",
    "            for key in obs_act:\n",
    "                obs_space = obs_act[key][0]\n",
    "                act_space = obs_act[key][1]\n",
    "\n",
    "                # Get waiting size\n",
    "                lane_sets = self.signal_config[key]['lane_sets']\n",
    "                lanes = []\n",
    "                for direction in lane_sets:\n",
    "                    for lane in lane_sets[direction]:\n",
    "                        if lane not in lanes: lanes.append(lane)\n",
    "                waits_len = len(lanes)\n",
    "\n",
    "                # Get fingerprint size\n",
    "                downstream = self.signal_config[key]['downstream']\n",
    "                neighbors = [downstream[direction] for direction in downstream]\n",
    "                fp_size = 0\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor is not None:\n",
    "                        fp_size += obs_act[neighbor][1]     # neighbor's action size\n",
    "\n",
    "                self.agents[key] = MA2CAgent(config, obs_space, act_space, fp_size, waits_len, 'ma2c'+key + str(thread_number), self.sess)\n",
    "\n",
    "            if sess is None:\n",
    "                self.saver = tf.train.Saver(max_to_keep=1)\n",
    "                self.sess.run(tf.global_variables_initializer())\n",
    "            else:\n",
    "                self.saver = None\n",
    "\n",
    "        def fingerprints(self, observation):\n",
    "            agent_fingerprint = {}\n",
    "            for agent_id in observation.keys():\n",
    "                downstream = self.signal_config[agent_id]['downstream']\n",
    "                neighbors = [downstream[direction] for direction in downstream]\n",
    "                fingerprints = []\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor is not None:\n",
    "                        neighbor_fp = self.agents[neighbor].fingerprint\n",
    "                        fingerprints.append(neighbor_fp)\n",
    "                agent_fingerprint[agent_id] = np.concatenate(fingerprints)\n",
    "            return agent_fingerprint\n",
    "\n",
    "        def act(self, observation):\n",
    "            acts = {}\n",
    "            fingerprints = self.fingerprints(observation)\n",
    "            for agent_id in observation.keys():\n",
    "                env_obs = observation[agent_id]\n",
    "                neighbor_fingerprints = fingerprints[agent_id]\n",
    "                combine = np.concatenate([env_obs, neighbor_fingerprints])\n",
    "\n",
    "                acts[agent_id] = self.agents[agent_id].act(combine)\n",
    "            return acts\n",
    "\n",
    "        def observe(self, observation, reward, done, info):\n",
    "            fingerprints = self.fingerprints(observation)\n",
    "            for agent_id in observation.keys():\n",
    "                env_obs = observation[agent_id]\n",
    "                neighbor_fingerprints = fingerprints[agent_id]\n",
    "                combine = np.concatenate([env_obs, neighbor_fingerprints])\n",
    "\n",
    "                agent = self.agents[agent_id]\n",
    "                agent.observe(combine, reward[agent_id], done, info)\n",
    "\n",
    "            if done:\n",
    "                if info['eps'] % 100 == 0:\n",
    "                    if self.saver is not None:\n",
    "                        self.saver.save(self.sess, self.config['log_dir']+'agent_' + 'checkpoint', global_step=info['eps'])\n",
    "\n",
    "\n",
    "    class MA2CAgent(Agent):\n",
    "        def __init__(self, config, observation_shape, num_actions, fingerprint_size, waits_len, name, sess):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.num_actions = num_actions\n",
    "            self.sess = sess\n",
    "\n",
    "            self.steps_done = 0\n",
    "            self.state = None\n",
    "            self.value = None\n",
    "            self.action = None\n",
    "            self.fingerprint = np.zeros(num_actions)\n",
    "\n",
    "            n_s = observation_shape[0] + fingerprint_size\n",
    "            n_a = num_actions\n",
    "            n_w = waits_len\n",
    "            n_f = fingerprint_size\n",
    "            total_step = config['steps']\n",
    "            model_config = config\n",
    "            print(name, n_s, n_a, n_w, n_f)\n",
    "\n",
    "            self.model = MA2CImplementation(n_s, n_a, n_w, n_f, total_step, model_config, name, sess)\n",
    "\n",
    "        def act(self, observation):\n",
    "            self.state = observation\n",
    "\n",
    "            policy, self.value = self.model.forward(observation, False)\n",
    "            self.action = np.random.choice(np.arange(len(policy)), p=policy)\n",
    "            self.fingerprint = np.array(policy)\n",
    "\n",
    "            return self.action\n",
    "\n",
    "        def observe(self, observation, reward, done, info):\n",
    "            self.model.add_transition(self.state, self.action, reward, self.value, done)\n",
    "            self.steps_done += 1\n",
    "\n",
    "            if self.steps_done % self.config['batch_size'] == 0 or done:\n",
    "                if done:\n",
    "                    R = 0\n",
    "                else:\n",
    "                    R = self.model.forward(observation, False, 'v')\n",
    "                self.model.backward(R)\n",
    "\n",
    "            if done:\n",
    "                self.steps_done = 0\n",
    "                self.model.reset()\n",
    "\n",
    "\n",
    "    # https://github.com/cts198859/deeprl_signal_control\n",
    "    class MA2CImplementation:\n",
    "        def __init__(self, n_s, n_a, n_w, n_f, total_step, model_config, name, sess):\n",
    "            self.name = name\n",
    "            self.sess = sess\n",
    "            self.reward_clip = model_config['reward_clip']\n",
    "            self.reward_norm = model_config['reward_norm']\n",
    "            self.n_s = n_s\n",
    "            self.n_a = n_a\n",
    "            self.n_f = n_f\n",
    "            self.n_w = n_w\n",
    "            self.n_step = model_config['batch_size']\n",
    "\n",
    "            # agent_name is needed to differentiate multi-agents\n",
    "            self.policy = self._init_policy(n_s - n_f - n_w, n_a, n_w, n_f, model_config, agent_name=name)\n",
    "\n",
    "            if total_step:\n",
    "                # training\n",
    "                self.total_step = total_step\n",
    "                self._init_scheduler(model_config)\n",
    "                self._init_train(model_config)\n",
    "\n",
    "        def _init_policy(self, n_s, n_a, n_w, n_f, model_config, agent_name=None):\n",
    "            n_fw = model_config['num_fw']\n",
    "            n_ft = model_config['num_ft']\n",
    "            n_lstm = model_config['num_lstm']\n",
    "            n_fp = model_config['num_fp']\n",
    "            policy = FPLstmACPolicy(n_s, n_a, n_w, n_f, self.n_step, n_fc_wave=n_fw,\n",
    "                                    n_fc_wait=n_ft, n_fc_fp=n_fp, n_lstm=n_lstm, name=agent_name)\n",
    "            return policy\n",
    "\n",
    "        def _init_scheduler(self, model_config):\n",
    "            lr_init = model_config['lr_init']\n",
    "            lr_decay = model_config['lr_decay']\n",
    "            beta_init = model_config['entropy_coef_init']\n",
    "            beta_decay = model_config['entropy_decay']\n",
    "            if lr_decay == 'constant':\n",
    "                self.lr_scheduler = Scheduler(lr_init, decay=lr_decay)\n",
    "            else:\n",
    "                lr_min = model_config['LR_MIN']\n",
    "                self.lr_scheduler = Scheduler(lr_init, lr_min, self.total_step, decay=lr_decay)\n",
    "            if beta_decay == 'constant':\n",
    "                self.beta_scheduler = Scheduler(beta_init, decay=beta_decay)\n",
    "            else:\n",
    "                beta_min = model_config['ENTROPY_COEF_MIN']\n",
    "                beta_ratio = model_config['ENTROPY_RATIO']\n",
    "                self.beta_scheduler = Scheduler(beta_init, beta_min, self.total_step * beta_ratio,\n",
    "                                                decay=beta_decay)\n",
    "\n",
    "        def _init_train(self, model_config):\n",
    "            # init loss\n",
    "            v_coef = model_config['value_coef']\n",
    "            max_grad_norm = model_config['max_grad_norm']\n",
    "            alpha = model_config['rmsp_alpha']\n",
    "            epsilon = model_config['rmsp_epsilon']\n",
    "            gamma = model_config['gamma']\n",
    "            self.policy.prepare_loss(v_coef, max_grad_norm, alpha, epsilon)\n",
    "            self.trans_buffer = OnPolicyBuffer(gamma)\n",
    "\n",
    "        def backward(self, R, summary_writer=None, global_step=None):\n",
    "            cur_lr = self.lr_scheduler.get(self.n_step)\n",
    "            cur_beta = self.beta_scheduler.get(self.n_step)\n",
    "            obs, acts, dones, Rs, Advs = self.trans_buffer.sample_transition(R)\n",
    "            self.policy.backward(self.sess, obs, acts, dones, Rs, Advs, cur_lr, cur_beta)\n",
    "\n",
    "        def forward(self, obs, done, out_type='pv'):\n",
    "            return self.policy.forward(self.sess, obs, done, out_type)\n",
    "\n",
    "        def reset(self):\n",
    "            self.policy._reset()\n",
    "\n",
    "        def add_transition(self, obs, actions, rewards, values, done):\n",
    "            if (self.reward_norm):\n",
    "                rewards = rewards / self.reward_norm\n",
    "            if self.reward_clip:\n",
    "                rewards = np.clip(rewards, -self.reward_clip, self.reward_clip)\n",
    "            self.trans_buffer.add_transition(obs, actions, rewards, values, done)\n",
    "\n",
    "        \"\"\"def load(self, model_dir, checkpoint=None):\n",
    "            save_file = None\n",
    "            save_step = 0\n",
    "            if os.path.exists(model_dir):\n",
    "                if checkpoint is None:\n",
    "                    for file in os.listdir(model_dir):\n",
    "                        if file.startswith('checkpoint'):\n",
    "                            prefix = file.split('.')[0]\n",
    "                            tokens = prefix.split('-')\n",
    "                            if len(tokens) != 2:\n",
    "                                continue\n",
    "                            cur_step = int(tokens[1])\n",
    "                            if cur_step > save_step:\n",
    "                                save_file = prefix\n",
    "                                save_step = cur_step\n",
    "                else:\n",
    "                    save_file = 'checkpoint-' + str(int(checkpoint))\n",
    "            if save_file is not None:\n",
    "                self.saver.restore(self.sess, model_dir + save_file)\n",
    "                logging.info('Checkpoint loaded: %s' % save_file)\n",
    "                return True\n",
    "            logging.error('Can not find old checkpoint for %s' % model_dir)\n",
    "            return False\"\"\"\n",
    "\n",
    "\n",
    "    class ACPolicy:\n",
    "        def __init__(self, n_a, n_s, n_step, policy_name, agent_name):\n",
    "            self.name = policy_name\n",
    "            if agent_name is not None:\n",
    "                # for multi-agent system\n",
    "                self.name += '_' + str(agent_name)\n",
    "            self.n_a = n_a\n",
    "            self.n_s = n_s\n",
    "            self.n_step = n_step\n",
    "\n",
    "        def forward(self, ob, *_args, **_kwargs):\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        def _build_out_net(self, h, out_type):\n",
    "            if out_type == 'pi':\n",
    "                pi = fc(h, out_type, self.n_a, act=tf.nn.softmax)\n",
    "                return tf.squeeze(pi)\n",
    "            else:\n",
    "                v = fc(h, out_type, 1, act=lambda x: x)\n",
    "                return tf.squeeze(v)\n",
    "\n",
    "        def _get_forward_outs(self, out_type):\n",
    "            outs = []\n",
    "            if 'p' in out_type:\n",
    "                outs.append(self.pi)\n",
    "            if 'v' in out_type:\n",
    "                outs.append(self.v)\n",
    "            return outs\n",
    "\n",
    "        def _return_forward_outs(self, out_values):\n",
    "            if len(out_values) == 1:\n",
    "                return out_values[0]\n",
    "            return out_values\n",
    "\n",
    "        def prepare_loss(self, v_coef, max_grad_norm, alpha, epsilon):\n",
    "            self.A = tf.placeholder(tf.int32, [self.n_step])\n",
    "            self.ADV = tf.placeholder(tf.float32, [self.n_step])\n",
    "            self.R = tf.placeholder(tf.float32, [self.n_step])\n",
    "            self.entropy_coef = tf.placeholder(tf.float32, [])\n",
    "            A_sparse = tf.one_hot(self.A, self.n_a)\n",
    "            log_pi = tf.log(tf.clip_by_value(self.pi, 1e-10, 1.0))\n",
    "            entropy = -tf.reduce_sum(self.pi * log_pi, axis=1)\n",
    "            entropy_loss = -tf.reduce_mean(entropy) * self.entropy_coef\n",
    "            policy_loss = -tf.reduce_mean(tf.reduce_sum(log_pi * A_sparse, axis=1) * self.ADV)\n",
    "            value_loss = tf.reduce_mean(tf.square(self.R - self.v)) * 0.5 * v_coef\n",
    "            self.loss = policy_loss + value_loss + entropy_loss\n",
    "\n",
    "            wts = tf.trainable_variables(scope=self.name)\n",
    "            grads = tf.gradients(self.loss, wts)\n",
    "            if max_grad_norm > 0:\n",
    "                grads, self.grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "            self.lr = tf.placeholder(tf.float32, [])\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.lr, decay=alpha,\n",
    "                                                       epsilon=epsilon)\n",
    "            self._train = self.optimizer.apply_gradients(list(zip(grads, wts)))\n",
    "            # monitor training\n",
    "            if self.name.endswith('_0a'):\n",
    "                summaries = []\n",
    "                # summaries.append(tf.summary.scalar('loss/%s_entropy_loss' % self.name, entropy_loss))\n",
    "                summaries.append(tf.summary.scalar('loss/%s_policy_loss' % self.name, policy_loss))\n",
    "                summaries.append(tf.summary.scalar('loss/%s_value_loss' % self.name, value_loss))\n",
    "                summaries.append(tf.summary.scalar('loss/%s_total_loss' % self.name, self.loss))\n",
    "                # summaries.append(tf.summary.scalar('train/%s_lr' % self.name, self.lr))\n",
    "                # summaries.append(tf.summary.scalar('train/%s_entropy_beta' % self.name, self.entropy_coef))\n",
    "                summaries.append(tf.summary.scalar('train/%s_gradnorm' % self.name, self.grad_norm))\n",
    "                self.summary = tf.summary.merge(summaries)\n",
    "\n",
    "\n",
    "    class LstmACPolicy(ACPolicy):\n",
    "        def __init__(self, n_s, n_a, n_w, n_step, n_fc_wave=128, n_fc_wait=32, n_lstm=64, name=None):\n",
    "            super().__init__(n_a, n_s, n_step, 'lstm', name)\n",
    "            self.n_lstm = n_lstm\n",
    "            self.n_fc_wait = n_fc_wait\n",
    "            self.n_fc_wave = n_fc_wave\n",
    "            self.n_w = n_w\n",
    "            self.ob_fw = tf.placeholder(tf.float32, [1, n_s + n_w])     # forward 1-step\n",
    "            self.done_fw = tf.placeholder(tf.float32, [1])\n",
    "            self.ob_bw = tf.placeholder(tf.float32, [n_step, n_s + n_w])     # backward n-step\n",
    "            self.done_bw = tf.placeholder(tf.float32, [n_step])\n",
    "            self.states = tf.placeholder(tf.float32, [2, n_lstm * 2])\n",
    "            with tf.variable_scope(self.name):\n",
    "                # pi and v use separate nets\n",
    "                self.pi_fw, pi_state = self._build_net('forward', 'pi')\n",
    "                self.v_fw, v_state = self._build_net('forward', 'v')\n",
    "                pi_state = tf.expand_dims(pi_state, 0)\n",
    "                v_state = tf.expand_dims(v_state, 0)\n",
    "                self.new_states = tf.concat([pi_state, v_state], 0)\n",
    "            with tf.variable_scope(self.name, reuse=True):\n",
    "                self.pi, _ = self._build_net('backward', 'pi')\n",
    "                self.v, _ = self._build_net('backward', 'v')\n",
    "            self._reset()\n",
    "\n",
    "        def _build_net(self, in_type, out_type):\n",
    "            if in_type == 'forward':\n",
    "                ob = self.ob_fw\n",
    "                done = self.done_fw\n",
    "            else:\n",
    "                ob = self.ob_bw\n",
    "                done = self.done_bw\n",
    "            if out_type == 'pi':\n",
    "                states = self.states[0]\n",
    "            else:\n",
    "                states = self.states[1]\n",
    "            if self.n_w == 0:\n",
    "                h = fc(ob, out_type + '_fcw', self.n_fc_wave)\n",
    "            else:\n",
    "                h0 = fc(ob[:, :self.n_s], out_type + '_fcw', self.n_fc_wave)\n",
    "                h1 = fc(ob[:, self.n_s:], out_type + '_fct', self.n_fc_wait)\n",
    "                h = tf.concat([h0, h1], 1)\n",
    "            h, new_states = lstm(h, done, states, out_type + '_lstm')\n",
    "            out_val = self._build_out_net(h, out_type)\n",
    "            return out_val, new_states\n",
    "\n",
    "        def _reset(self):\n",
    "            # forget the cumulative states every cum_step\n",
    "            self.states_fw = np.zeros((2, self.n_lstm * 2), dtype=np.float32)\n",
    "            self.states_bw = np.zeros((2, self.n_lstm * 2), dtype=np.float32)\n",
    "\n",
    "        def forward(self, sess, ob, done, out_type='pv'):\n",
    "            outs = self._get_forward_outs(out_type)\n",
    "            # update state only when p is called\n",
    "            if 'p' in out_type:\n",
    "                outs.append(self.new_states)\n",
    "            out_values = sess.run(outs, {self.ob_fw: np.array([ob]),\n",
    "                                         self.done_fw: np.array([done]),\n",
    "                                         self.states: self.states_fw})\n",
    "            if 'p' in out_type:\n",
    "                self.states_fw = out_values[-1]\n",
    "                out_values = out_values[:-1]\n",
    "            return self._return_forward_outs(out_values)\n",
    "\n",
    "        def backward(self, sess, obs, acts, dones, Rs, Advs, cur_lr, cur_beta,\n",
    "                     summary_writer=None, global_step=None):\n",
    "            if summary_writer is None:\n",
    "                ops = self._train\n",
    "            else:\n",
    "                ops = [self.summary, self._train]\n",
    "            outs = sess.run(ops,\n",
    "                            {self.ob_bw: obs,\n",
    "                             self.done_bw: dones,\n",
    "                             self.states: self.states_bw,\n",
    "                             self.A: acts,\n",
    "                             self.ADV: Advs,\n",
    "                             self.R: Rs,\n",
    "                             self.lr: cur_lr,\n",
    "                             self.entropy_coef: cur_beta})\n",
    "            self.states_bw = np.copy(self.states_fw)\n",
    "            if summary_writer is not None:\n",
    "                summary_writer.add_summary(outs[0], global_step=global_step)\n",
    "\n",
    "        def _get_forward_outs(self, out_type):\n",
    "            outs = []\n",
    "            if 'p' in out_type:\n",
    "                outs.append(self.pi_fw)\n",
    "            if 'v' in out_type:\n",
    "                outs.append(self.v_fw)\n",
    "            return outs\n",
    "\n",
    "\n",
    "    class FPLstmACPolicy(LstmACPolicy):\n",
    "        def __init__(self, n_s, n_a, n_w, n_f, n_step, n_fc_wave=128, n_fc_wait=32, n_fc_fp=32, n_lstm=64, name=None):\n",
    "            ACPolicy.__init__(self, n_a, n_s, n_step, 'fplstm', name)\n",
    "            self.n_lstm = n_lstm\n",
    "            self.n_fc_wave = n_fc_wave\n",
    "            self.n_fc_wait = n_fc_wait\n",
    "            self.n_fc_fp = n_fc_fp\n",
    "            self.n_w = n_w\n",
    "            self.ob_fw = tf.placeholder(tf.float32, [1, n_s + n_w + n_f])   # forward 1-step\n",
    "            self.done_fw = tf.placeholder(tf.float32, [1])\n",
    "            self.ob_bw = tf.placeholder(tf.float32, [n_step, n_s + n_w + n_f])  # backward n-step\n",
    "            self.done_bw = tf.placeholder(tf.float32, [n_step])\n",
    "            self.states = tf.placeholder(tf.float32, [2, n_lstm * 2])\n",
    "            with tf.variable_scope(self.name):\n",
    "                # pi and v use separate nets\n",
    "                self.pi_fw, pi_state = self._build_net('forward', 'pi')\n",
    "                self.v_fw, v_state = self._build_net('forward', 'v')\n",
    "                pi_state = tf.expand_dims(pi_state, 0)\n",
    "                v_state = tf.expand_dims(v_state, 0)\n",
    "                self.new_states = tf.concat([pi_state, v_state], 0)\n",
    "            with tf.variable_scope(self.name, reuse=True):\n",
    "                self.pi, _ = self._build_net('backward', 'pi')\n",
    "                self.v, _ = self._build_net('backward', 'v')\n",
    "            self._reset()\n",
    "\n",
    "        def _build_net(self, in_type, out_type):\n",
    "            if in_type == 'forward':\n",
    "                ob = self.ob_fw\n",
    "                done = self.done_fw\n",
    "            else:\n",
    "                ob = self.ob_bw\n",
    "                done = self.done_bw\n",
    "            if out_type == 'pi':\n",
    "                states = self.states[0]\n",
    "            else:\n",
    "                states = self.states[1]\n",
    "            h0 = fc(ob[:, :self.n_s], out_type + '_fcw', self.n_fc_wave)\n",
    "            h1 = fc(ob[:, (self.n_s + self.n_w):], out_type + '_fcf', self.n_fc_fp)\n",
    "            if self.n_w == 0:\n",
    "                h = tf.concat([h0, h1], 1)\n",
    "            else:\n",
    "                h2 = fc(ob[:, self.n_s: (self.n_s + self.n_w)], out_type + '_fct', self.n_fc_wait)\n",
    "                h = tf.concat([h0, h1, h2], 1)\n",
    "            h, new_states = lstm(h, done, states, out_type + '_lstm')\n",
    "            out_val = self._build_out_net(h, out_type)\n",
    "            return out_val, new_states\n",
    "\n",
    "\n",
    "    DEFAULT_SCALE = np.sqrt(2)\n",
    "    DEFAULT_MODE = 'fan_in'\n",
    "\n",
    "\n",
    "    def ortho_init(scale=DEFAULT_SCALE, mode=None):\n",
    "        def _ortho_init(shape, dtype, partition_info=None):\n",
    "            # lasagne ortho init for tf\n",
    "            shape = tuple(shape)\n",
    "            if len(shape) == 2:     # fc: in, out\n",
    "                flat_shape = shape\n",
    "            elif (len(shape) == 3) or (len(shape) == 4):    # 1d/2dcnn: (in_h), in_w, in_c, out\n",
    "                flat_shape = (np.prod(shape[:-1]), shape[-1])\n",
    "            a = np.random.standard_normal(flat_shape)\n",
    "            u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "            q = u if u.shape == flat_shape else v   # pick the one with the correct shape\n",
    "            q = q.reshape(shape)\n",
    "            return (scale * q).astype(np.float32)\n",
    "        return _ortho_init\n",
    "\n",
    "\n",
    "    DEFAULT_METHOD = ortho_init\n",
    "\n",
    "\n",
    "    def fc(x, scope, n_out, act=tf.nn.relu, init_scale=DEFAULT_SCALE,\n",
    "           init_mode=DEFAULT_MODE, init_method=DEFAULT_METHOD):\n",
    "        with tf.variable_scope(scope):\n",
    "            n_in = x.shape[1].value\n",
    "            w = tf.get_variable(\"w\", [n_in, n_out],\n",
    "                                initializer=init_method(init_scale, init_mode))\n",
    "            b = tf.get_variable(\"b\", [n_out], initializer=tf.constant_initializer(0.0))\n",
    "            z = tf.matmul(x, w) + b\n",
    "            return act(z)\n",
    "\n",
    "\n",
    "    def batch_to_seq(x):\n",
    "        n_step = x.shape[0].value\n",
    "        if len(x.shape) == 1:\n",
    "            x = tf.expand_dims(x, -1)\n",
    "        return tf.split(axis=0, num_or_size_splits=n_step, value=x)\n",
    "\n",
    "\n",
    "    def seq_to_batch(x):\n",
    "        return tf.concat(axis=0, values=x)\n",
    "\n",
    "\n",
    "    def lstm(xs, dones, s, scope, init_scale=DEFAULT_SCALE, init_mode=DEFAULT_MODE,\n",
    "             init_method=DEFAULT_METHOD):\n",
    "        xs = batch_to_seq(xs)\n",
    "        # need dones to reset states\n",
    "        dones = batch_to_seq(dones)\n",
    "        n_in = xs[0].shape[1].value\n",
    "        n_out = s.shape[0] // 2\n",
    "        with tf.variable_scope(scope):\n",
    "            wx = tf.get_variable(\"wx\", [n_in, n_out*4],\n",
    "                                 initializer=init_method(init_scale, init_mode))\n",
    "            wh = tf.get_variable(\"wh\", [n_out, n_out*4],\n",
    "                                 initializer=init_method(init_scale, init_mode))\n",
    "            b = tf.get_variable(\"b\", [n_out*4], initializer=tf.constant_initializer(0.0))\n",
    "        s = tf.expand_dims(s, 0)\n",
    "        c, h = tf.split(axis=1, num_or_size_splits=2, value=s)\n",
    "        for ind, (x, done) in enumerate(zip(xs, dones)):\n",
    "            c = c * (1-done)\n",
    "            h = h * (1-done)\n",
    "            z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n",
    "            i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)\n",
    "            i = tf.nn.sigmoid(i)\n",
    "            f = tf.nn.sigmoid(f)\n",
    "            o = tf.nn.sigmoid(o)\n",
    "            u = tf.tanh(u)\n",
    "            c = f*c + i*u\n",
    "            h = o*tf.tanh(c)\n",
    "            xs[ind] = h\n",
    "        s = tf.concat(axis=1, values=[c, h])\n",
    "        return seq_to_batch(xs), tf.squeeze(s)\n",
    "\n",
    "\n",
    "    class Scheduler:\n",
    "        def __init__(self, val_init, val_min=0, total_step=0, decay='linear'):\n",
    "            self.val = val_init\n",
    "            self.N = float(total_step)\n",
    "            self.val_min = val_min\n",
    "            self.decay = decay\n",
    "            self.n = 0\n",
    "\n",
    "        def get(self, n_step):\n",
    "            self.n += n_step\n",
    "            if self.decay == 'linear':\n",
    "                return max(self.val_min, self.val * (1 - self.n / self.N))\n",
    "            else:\n",
    "                return self.val\n",
    "\n",
    "\n",
    "    class TransBuffer:\n",
    "        def reset(self):\n",
    "            self.buffer = []\n",
    "\n",
    "        @property\n",
    "        def size(self):\n",
    "            return len(self.buffer)\n",
    "\n",
    "        def add_transition(self, ob, a, r, *_args, **_kwargs):\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        def sample_transition(self, *_args, **_kwargs):\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "    class OnPolicyBuffer(TransBuffer):\n",
    "        def __init__(self, gamma):\n",
    "            self.gamma = gamma\n",
    "            self.reset()\n",
    "\n",
    "        def reset(self, done=False):\n",
    "            # the done before each step is required\n",
    "            self.obs = []\n",
    "            self.acts = []\n",
    "            self.rs = []\n",
    "            self.vs = []\n",
    "            self.dones = [done]\n",
    "\n",
    "        def add_transition(self, ob, a, r, v, done):\n",
    "            self.obs.append(ob)\n",
    "            self.acts.append(a)\n",
    "            self.rs.append(r)\n",
    "            self.vs.append(v)\n",
    "            self.dones.append(done)\n",
    "\n",
    "        def _add_R_Adv(self, R):\n",
    "            Rs = []\n",
    "            Advs = []\n",
    "            # use post-step dones here\n",
    "            for r, v, done in zip(self.rs[::-1], self.vs[::-1], self.dones[:0:-1]):\n",
    "                R = r + self.gamma * R * (1.-done)\n",
    "                Adv = R - v\n",
    "                Rs.append(R)\n",
    "                Advs.append(Adv)\n",
    "            Rs.reverse()\n",
    "            Advs.reverse()\n",
    "            self.Rs = Rs\n",
    "            self.Advs = Advs\n",
    "\n",
    "        def sample_transition(self, R, discrete=True):\n",
    "            self._add_R_Adv(R)\n",
    "            obs = np.array(self.obs, dtype=np.float32)\n",
    "            if discrete:\n",
    "                acts = np.array(self.acts, dtype=np.int32)\n",
    "            else:\n",
    "                acts = np.array(self.acts, dtype=np.float32)\n",
    "            Rs = np.array(self.Rs, dtype=np.float32)\n",
    "            Advs = np.array(self.Advs, dtype=np.float32)\n",
    "            # use pre-step dones here\n",
    "            dones = np.array(self.dones[:-1], dtype=np.bool)\n",
    "            self.reset(self.dones[-1])\n",
    "            return obs, acts, dones, Rs, Advs\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
