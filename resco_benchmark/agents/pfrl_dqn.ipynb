{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891578ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pfrl\n",
    "from pfrl import explorers, replay_buffers\n",
    "from pfrl.explorer import Explorer\n",
    "from pfrl.agents import DQN\n",
    "from pfrl.q_functions import DiscreteActionValueHead\n",
    "from pfrl.utils.contexts import evaluating\n",
    "\n",
    "from resco_benchmark.agents.agent import IndependentAgent, Agent\n",
    "\n",
    "\n",
    "class IDQN(IndependentAgent):\n",
    "    def __init__(self, config, obs_act, map_name, thread_number):\n",
    "        super().__init__(config, obs_act, map_name, thread_number)\n",
    "        for key in obs_act:\n",
    "            obs_space = obs_act[key][0]\n",
    "            act_space = obs_act[key][1]\n",
    "\n",
    "            def conv2d_size_out(size, kernel_size=2, stride=1):\n",
    "                return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "            h = conv2d_size_out(obs_space[1])\n",
    "            w = conv2d_size_out(obs_space[2])\n",
    "\n",
    "            model = nn.Sequential(\n",
    "                nn.Conv2d(obs_space[0], 64, kernel_size=(2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(h * w * 64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, act_space),\n",
    "                DiscreteActionValueHead()\n",
    "            )\n",
    "\n",
    "            self.agents[key] = DQNAgent(config, act_space, model)\n",
    "            if self.config['load']:\n",
    "                print('LOADING SAVED MODEL FOR EVALUATION')\n",
    "                self.agents[key].load(self.config['log_dir']+'agent_'+key+'.pt')\n",
    "                self.agents[key].agent.training = False\n",
    "\n",
    "\n",
    "class DQNAgent(Agent):\n",
    "    def __init__(self, config, act_space, model, num_agents=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        replay_buffer = replay_buffers.ReplayBuffer(10000)\n",
    "\n",
    "        if num_agents > 0:\n",
    "            explorer = SharedEpsGreedy(\n",
    "                config['EPS_START'],\n",
    "                config['EPS_END'],\n",
    "                num_agents*config['steps'],\n",
    "                lambda: np.random.randint(act_space),\n",
    "            )\n",
    "        else:\n",
    "            explorer = explorers.LinearDecayEpsilonGreedy(\n",
    "                config['EPS_START'],\n",
    "                config['EPS_END'],\n",
    "                config['steps'],\n",
    "                lambda: np.random.randint(act_space),\n",
    "            )\n",
    "\n",
    "        if num_agents > 0:\n",
    "            print('USING SHAREDDQN')\n",
    "            self.agent = SharedDQN(self.model, self.optimizer, replay_buffer,\n",
    "                                   config['GAMMA'], explorer, gpu=self.device.index,\n",
    "                                   minibatch_size=config['BATCH_SIZE'], replay_start_size=config['BATCH_SIZE'],\n",
    "                                   phi=lambda x: np.asarray(x, dtype=np.float32),\n",
    "                                   target_update_interval=config['TARGET_UPDATE']*num_agents, update_interval=num_agents)\n",
    "        else:\n",
    "            self.agent = DQN(self.model, self.optimizer, replay_buffer, config['GAMMA'], explorer,\n",
    "                             gpu=self.device.index,\n",
    "                             minibatch_size=config['BATCH_SIZE'], replay_start_size=config['BATCH_SIZE'],\n",
    "                             phi=lambda x: np.asarray(x, dtype=np.float32),\n",
    "                             target_update_interval=config['TARGET_UPDATE'])\n",
    "\n",
    "    def act(self, observation, valid_acts=None, reverse_valid=None):\n",
    "        if isinstance(self.agent, SharedDQN):\n",
    "            return self.agent.act(observation, valid_acts=valid_acts, reverse_valid=reverse_valid)\n",
    "        else:\n",
    "            return self.agent.act(observation)\n",
    "\n",
    "    def observe(self, observation, reward, done, info):\n",
    "        if isinstance(self.agent, SharedDQN):\n",
    "            self.agent.observe(observation, reward, done, info)\n",
    "        else:\n",
    "            self.agent.observe(observation, reward, done, False)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path+'.pt')\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path)['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(torch.load(path)['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "class SharedDQN(DQN):\n",
    "    def __init__(self, q_function: torch.nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                 replay_buffer: pfrl.replay_buffer.AbstractReplayBuffer, gamma: float, explorer: Explorer,\n",
    "                 gpu, minibatch_size, replay_start_size, phi, target_update_interval, update_interval):\n",
    "\n",
    "        super().__init__(q_function, optimizer, replay_buffer, gamma, explorer,\n",
    "                         gpu=gpu, minibatch_size=minibatch_size, replay_start_size=replay_start_size, phi=phi,\n",
    "                         target_update_interval=target_update_interval, update_interval=update_interval)\n",
    "\n",
    "    def act(self, obs: Any, valid_acts=None, reverse_valid=None) -> Any:\n",
    "        return self.batch_act(obs, valid_acts=valid_acts, reverse_valid=reverse_valid)\n",
    "\n",
    "    def observe(self, obs: Sequence[Any], reward: Sequence[float], done: Sequence[bool], reset: Sequence[bool]) -> None:\n",
    "        self.batch_observe(obs, reward, done, reset)\n",
    "\n",
    "    def batch_act(self, batch_obs: Sequence[Any], valid_acts=None, reverse_valid=None) -> Sequence[Any]:\n",
    "        if valid_acts is None: return super(SharedDQN, self).batch_act(batch_obs)\n",
    "        with torch.no_grad(), evaluating(self.model):\n",
    "            batch_av = self._evaluate_model_and_update_recurrent_states(batch_obs)\n",
    "\n",
    "            batch_qvals = batch_av.params[0].detach().cpu().numpy()\n",
    "            batch_argmax = []\n",
    "            for i in range(len(batch_obs)):\n",
    "                batch_item = batch_qvals[i]\n",
    "                max_val, max_idx = None, None\n",
    "                for idx in valid_acts[i]:\n",
    "                    batch_item_qval = batch_item[idx]\n",
    "                    if max_val is None:\n",
    "                        max_val = batch_item_qval\n",
    "                        max_idx = idx\n",
    "                    elif batch_item_qval > max_val:\n",
    "                        max_val = batch_item_qval\n",
    "                        max_idx = idx\n",
    "                batch_argmax.append(max_idx)\n",
    "            batch_argmax = np.asarray(batch_argmax)\n",
    "\n",
    "        if self.training:\n",
    "            batch_action = []\n",
    "            for i in range(len(batch_obs)):\n",
    "                av = batch_av[i : i + 1]\n",
    "                greed = batch_argmax[i]\n",
    "                act, greedy = self.explorer.select_action(self.t, lambda: greed, action_value=av, num_acts=len(valid_acts[i]))\n",
    "                if not greedy:\n",
    "                    act = reverse_valid[i][act]\n",
    "                batch_action.append(act)\n",
    "\n",
    "            self.batch_last_obs = list(batch_obs)\n",
    "            self.batch_last_action = list(batch_action)\n",
    "        else:\n",
    "            batch_action = batch_argmax\n",
    "\n",
    "        valid_batch_action = []\n",
    "        for i in range(len(batch_action)):\n",
    "            valid_batch_action.append(valid_acts[i][batch_action[i]])\n",
    "        return valid_batch_action\n",
    "\n",
    "\n",
    "def select_action_epsilon_greedily(epsilon, random_action_func, greedy_action_func):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random_action_func(), False\n",
    "    else:\n",
    "        return greedy_action_func(), True\n",
    "\n",
    "\n",
    "class SharedEpsGreedy(explorers.LinearDecayEpsilonGreedy):\n",
    "\n",
    "    def select_action(self, t, greedy_action_func, action_value=None, num_acts=None):\n",
    "        self.epsilon = self.compute_epsilon(t)\n",
    "        if num_acts is None:\n",
    "            fn = self.random_action_func\n",
    "        else:\n",
    "            fn = lambda: np.random.randint(num_acts)\n",
    "        a, greedy = select_action_epsilon_greedily(\n",
    "            self.epsilon, fn, greedy_action_func\n",
    "        )\n",
    "        greedy_str = \"greedy\" if greedy else \"non-greedy\"\n",
    "        self.logger.debug(\"t:%s a:%s %s\", t, a, greedy_str)\n",
    "        if num_acts is None:\n",
    "            return a\n",
    "        else:\n",
    "            return a, greedy"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
